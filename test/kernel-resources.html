<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>19.4. カーネルリソースの管理</title><link rel="stylesheet" type="text/css" href="stylesheet.css" /><link rev="made" href="pgsql-docs@lists.postgresql.org" /><meta name="generator" content="DocBook XSL Stylesheets Vsnapshot" /><link rel="prev" href="server-start.html" title="19.3. データベースサーバの起動" /><link rel="next" href="server-shutdown.html" title="19.5. サーバのシャットダウン" /><meta name="viewport" content="width=device-width,initial-scale=1.0" /></head><body id="docContent" class="container-fluid col-10"><div class="navheader"><table width="100%" summary="Navigation header"><tr><th colspan="4" align="center"><a accesskey="h" href="index.html">PostgreSQL 16.0文書</a></th></tr><tr><td width="10%" align="left"></td><td width="10%" align="left"></td><td width="60%" align="center"><a href="runtime.html" title="第19章 サーバの準備と運用">第19章 サーバの準備と運用</a></td><td width="20%" align="right"></td></tr><tr><td width="10%" align="left"><a accesskey="p" href="server-start.html" title="19.3. データベースサーバの起動">前へ</a> </td><td width="10%" align="left"><a accesskey="u" href="runtime.html" title="第19章 サーバの準備と運用">上へ</a></td><td width="60%" align="center">19.4. カーネルリソースの管理</td><td width="20%" align="right"> <a accesskey="n" href="server-shutdown.html" title="19.5. サーバのシャットダウン">次へ</a></td></tr></table><hr /></div><div class="sect1" id="KERNEL-RESOURCES"><div class="titlepage"><div><div><h2 class="title" style="clear: both">19.4. カーネルリソースの管理 <a href="#KERNEL-RESOURCES" class="id_link">#</a></h2></div></div></div><!--
  <title>Managing Kernel Resources</title>
--><p>
<!--
   <productname>PostgreSQL</productname> can sometimes exhaust various operating system
   resource limits, especially when multiple copies of the server are running
   on the same system, or in very large installations.  This section explains
   the kernel resources used by <productname>PostgreSQL</productname> and the steps you
   can take to resolve problems related to kernel resource consumption.
-->
<span class="productname">PostgreSQL</span>は、特に同一システム上で複数のサーバコピーを実行している場合や非常に大規模なインストレーションでは、オペレーティングシステムの様々なリソース制限を超えてしまうことがあります。
本節では、<span class="productname">PostgreSQL</span>で使用されるカーネルリソース、およびカーネルリソース消費に関連した問題を解消する時に取ることができる手順について説明します。
  </p><div class="sect2" id="SYSVIPC"><div class="titlepage"><div><div><h3 class="title">19.4.1. 共有メモリとセマフォ <a href="#SYSVIPC" class="id_link">#</a></h3></div></div></div><!--
   <title>Shared Memory and Semaphores</title>
--><a id="id-1.6.6.7.3.2" class="indexterm"></a><a id="id-1.6.6.7.3.3" class="indexterm"></a><a id="id-1.6.6.7.3.4" class="indexterm"></a><a id="id-1.6.6.7.3.5" class="indexterm"></a><p>
<!--
    <productname>PostgreSQL</productname> requires the operating system to provide
    inter-process communication (<acronym>IPC</acronym>) features, specifically
    shared memory and semaphores.  Unix-derived systems typically provide
    <quote><systemitem class="osname">System V</systemitem></quote> <acronym>IPC</acronym>,
    <quote><systemitem class="osname">POSIX</systemitem></quote> <acronym>IPC</acronym>, or both.
    <systemitem class="osname">Windows</systemitem> has its own implementation of
    these features and is not discussed here.
-->
<span class="productname">PostgreSQL</span>はオペレーティングシステムが、プロセス間通信(<acronym class="acronym">IPC</acronym>)特に共有メモリとセマフォ機能を提供することを要求します。
Unix（派生）システムでは、<span class="quote">「<span class="quote"><span class="systemitem">System V</span></span>」</span> <acronym class="acronym">IPC</acronym>や、<span class="quote">「<span class="quote"><span class="systemitem">POSIX</span></span>」</span> <acronym class="acronym">IPC</acronym>、またはその両方を提供します。
<span class="systemitem">Windows</span>は、これらの機能を独自で実装しているため、ここでは説明しません。
   </p><p>
<!--
    By default, <productname>PostgreSQL</productname> allocates
    a very small amount of System V shared memory, as well as a much larger
    amount of anonymous <function>mmap</function> shared memory.
    Alternatively, a single large System V shared memory region can be used
    (see <xref linkend="guc-shared-memory-type"/>).

    In addition a significant number of semaphores, which can be either
    System V or POSIX style, are created at server startup.  Currently,
    POSIX semaphores are used on Linux and FreeBSD systems while other
    platforms use System V semaphores.
-->
デフォルトでは<span class="productname">PostgreSQL</span>は通常、非常に少量のSystem V共有メモリと、もっと大量の無名<code class="function">mmap</code>共有メモリを割り当てます。
代替方法として、単一の大きなSystem Vメモリリージョンも利用できます
（<a class="xref" href="runtime-config-resource.html#GUC-SHARED-MEMORY-TYPE">shared_memory_type</a>参照）。

さらに、System V又はPOSIXスタイルのどちらかのセマフォがサーバの起動時に作成されます。
現在、LinuxとFreeBSDシステムではPOSIXセマフォが使用され、それ以外のプラットフォームではSystem Vセマフォが使用されます。
   </p><p>
<!--
    System V <acronym>IPC</acronym> features are typically constrained by
    system-wide allocation limits.
    When <productname>PostgreSQL</productname> exceeds one of these limits,
    the server will refuse to start and
    should leave an instructive error message describing the problem
    and what to do about it. (See also <xref
    linkend="server-start-failures"/>.) The relevant kernel
    parameters are named consistently across different systems; <xref
    linkend="sysvipc-parameters"/> gives an overview. The methods to set
    them, however, vary. Suggestions for some platforms are given below.
-->
System V <acronym class="acronym">IPC</acronym>機能は、通常システム全体の割り当て制限に制約されます。
<span class="productname">PostgreSQL</span>がこれらの制限のいずれかを超えると、サーバは起動を拒否し、問題および何をすべきかを説明するエラーメッセージを残します。
（<a class="xref" href="server-start.html#SERVER-START-FAILURES" title="19.3.1. サーバ起動の失敗">19.3.1</a> も参照してください。）
関係するカーネルパラメータは別々のシステム上でも統一して名付けられています。
<a class="xref" href="kernel-resources.html#SYSVIPC-PARAMETERS" title="表19.1 System V IPCパラメータ">表 19.1</a>で概略がわかります。
しかしこれらを設定するための方法は異なります。
以下に、いくつかのプラットフォームへの提案を挙げます。
   </p><div class="table" id="SYSVIPC-PARAMETERS"><p class="title"><strong>表19.1 <span class="systemitem">System V</span> <acronym class="acronym">IPC</acronym>パラメータ</strong></p><div class="table-contents"><!--
    <title><systemitem class="osname">System V</systemitem> <acronym>IPC</acronym> Parameters</title>
--><table class="table" summary="System V IPCパラメータ" border="1"><colgroup><col class="col1" /><col class="col2" /><col class="col3" /></colgroup><thead><tr><th>名前</th><th>説明</th><th>一つの<span class="productname">PostgreSQL</span>インスタンスに必要な値</th></tr></thead><tbody><tr><td><code class="varname">SHMMAX</code></td><td>共有メモリセグメントの最大サイズ（バイト）</td><td>最小でも1キロバイト（ただしデフォルトはもっと多くなっています）</td></tr><tr><td><code class="varname">SHMMIN</code></td><td>共有メモリセグメントの最小サイズ（バイト）</td><td>1</td></tr><tr><td><code class="varname">SHMALL</code></td><td>使用可能な共有メモリの総量（バイトまたはページ）</td><td>バイト指定の場合は<code class="varname">SHMMAX</code>と同じ。ページ指定の場合は<code class="literal">ceil(SHMMAX/PAGE_SIZE)</code>。 + 他のアプリケーション用の空間</td></tr><tr><td><code class="varname">SHMSEG</code></td><td>プロセスごとの共有メモリセグメントの最大数</td><td>必要なのは1セグメントのみ（ただしデフォルトはもっと多くなっています）</td></tr><tr><td><code class="varname">SHMMNI</code></td><td>システム全体の共有メモリセグメントの最大数</td><td><code class="varname">SHMSEG</code>と同様 + 他のアプリケーション用の空間</td></tr><tr><td><code class="varname">SEMMNI</code></td><td>セマフォ識別子の最大数（つまりセット）</td><td>最低<code class="literal">ceil((max_connections + autovacuum_max_workers + max_worker_processes + 5) / 16)</code> + 他のアプリケーション用の空間</td></tr><tr><td><code class="varname">SEMMNS</code></td><td>システム全体のセマフォの最大数</td><td><code class="literal">ceil((max_connections + autovacuum_max_workers + max_worker_processes + 5) / 16) * 17</code> + 他のアプリケーション用の空間</td></tr><tr><td><code class="varname">SEMMSL</code></td><td>セットごとのセマフォの最大数</td><td>最低17</td></tr><tr><td><code class="varname">SEMMAP</code></td><td>セマフォマップの中の項目の数</td><td>本文を参照</td></tr><tr><td><code class="varname">SEMVMX</code></td><td>セマフォの最大値</td><td>最低1000（デフォルトはしばしば32767ですが、必要がなければ変更しないでください）</td></tr></tbody></table></div></div><br class="table-break" /><p>
<!--
    <productname>PostgreSQL</productname> requires a few bytes of System V shared memory
    (typically 48 bytes, on 64-bit platforms) for each copy of the server.
    On most modern operating systems, this amount can easily be allocated.
    However, if you are running many copies of the server or you explicitly
    configure the server to use large amounts of System V shared memory (see
    <xref linkend="guc-shared-memory-type"/> and <xref
    linkend="guc-dynamic-shared-memory-type"/>), it may be necessary to
    increase <varname>SHMALL</varname>, which is the total amount of System V shared
    memory system-wide.  Note that <varname>SHMALL</varname> is measured in pages
    rather than bytes on many systems.
-->
<span class="productname">PostgreSQL</span>は、サーバのコピー毎にSystem V共有メモリの数バイト（64ビットプラットフォームでは通常48バイト）を必要とします。
最近のほとんどのオペレーティングシステムでは、このくらいの量は簡単に割り当てられます。
しかし複数のサーバのコピーを実行している場合やSystem V共有メモリを使用する他のアプリケーションを実行している場合（<a class="xref" href="runtime-config-resource.html#GUC-SHARED-MEMORY-TYPE">shared_memory_type</a>および<a class="xref" href="runtime-config-resource.html#GUC-DYNAMIC-SHARED-MEMORY-TYPE">dynamic_shared_memory_type</a>を参照）は、システム全体のSystem V共有メモリである<code class="varname">SHMALL</code>を増加させる必要があるかもしれません。
多くのシステムでは<code class="varname">SHMALL</code>をバイト単位ではなくページ単位で測ることに注意してください。
   </p><p>
<!--
    Less likely to cause problems is the minimum size for shared
    memory segments (<varname>SHMMIN</varname>), which should be at most
    approximately 32 bytes for <productname>PostgreSQL</productname> (it is
    usually just 1). The maximum number of segments system-wide
    (<varname>SHMMNI</varname>) or per-process (<varname>SHMSEG</varname>) are unlikely
    to cause a problem unless your system has them set to zero.
-->
問題が少ないのは共有メモリセグメントの最小サイズ（<code class="varname">SHMMIN</code>）で、<span class="productname">PostgreSQL</span>では最大でもおよそ32バイトのはずです（通常では1です）。
システム全体のセグメントの最大数（<code class="varname">SHMMNI</code>）もしくはプロセスごとのセグメントの最大数（<code class="varname">SHMSEG</code>）に関して、使用しているシステムで0に設定されていない限り、問題が起きることはほぼありません。
   </p><p>
<!--
    When using System V semaphores,
    <productname>PostgreSQL</productname> uses one semaphore per allowed connection
    (<xref linkend="guc-max-connections"/>), allowed autovacuum worker process
    (<xref linkend="guc-autovacuum-max-workers"/>) and allowed background
    process (<xref linkend="guc-max-worker-processes"/>), in sets of 16.
    Each such set will
    also contain a 17th semaphore which contains a <quote>magic
    number</quote>, to detect collision with semaphore sets used by
    other applications. The maximum number of semaphores in the system
    is set by <varname>SEMMNS</varname>, which consequently must be at least
    as high as <varname>max_connections</varname> plus
    <varname>autovacuum_max_workers</varname> plus <varname>max_wal_senders</varname>,
    plus <varname>max_worker_processes</varname>, plus one extra for each 16
    allowed connections plus workers (see the formula in <xref
    linkend="sysvipc-parameters"/>).  The parameter <varname>SEMMNI</varname>
    determines the limit on the number of semaphore sets that can
    exist on the system at one time.  Hence this parameter must be at
    least <literal>ceil((max_connections + autovacuum_max_workers + max_wal_senders + max_worker_processes + 5) / 16)</literal>.
    Lowering the number
    of allowed connections is a temporary workaround for failures,
    which are usually confusingly worded <quote>No space
    left on device</quote>, from the function <function>semget</function>.
-->
System V セマフォを使用している場合、<span class="productname">PostgreSQL</span>は、許可した接続（<a class="xref" href="runtime-config-connection.html#GUC-MAX-CONNECTIONS">max_connections</a>）、許可したオートバキュームワーカープロセス（<a class="xref" href="runtime-config-autovacuum.html#GUC-AUTOVACUUM-MAX-WORKERS">autovacuum_max_workers</a>）、許可したバックエンドプロセス(<a class="xref" href="runtime-config-resource.html#GUC-MAX-WORKER-PROCESSES">max_worker_processes</a>)ごとに1つのセマフォを使用し、16個のセマフォをセットとして扱います。
それぞれそのようなセットは、他のアプリケーションに使われているセマフォセットとの衝突を検出するための<span class="quote">「<span class="quote">マジックナンバー</span>」</span>が含まれている17個目のセマフォを持っています。
システム内のセマフォの最大数は<code class="varname">SEMMNS</code>によって設定され、その結果としてその値は少なくとも<code class="varname">max_connections</code>＋<code class="varname">autovacuum_max_workers</code>＋<code class="varname">max_wal_senders</code>＋<code class="varname">max_worker_processes</code>と同じ、ただし、許可された接続とワーカー16個ごとに余分な1個を加えた値以上はなければいけません（<a class="xref" href="kernel-resources.html#SYSVIPC-PARAMETERS" title="表19.1 System V IPCパラメータ">表 19.1</a>の公式を参照してください）。
<code class="varname">SEMMNI</code>パラメータはシステム上に同時に存在できるセマフォ集合の数の上限を決定します。
ですから、このパラメータは少なくとも<code class="literal">ceil((max_connections + autovacuum_max_workers  + max_wal_senders + max_worker_processes + 5) / 16)</code>以上はなくてはいけません。
一時的な失敗の回避策としては許可される接続の数を下げることができますが、<span class="quote">「<span class="quote">No space left on device</span>」</span>という紛らわしい言葉が<code class="function">semget</code>関数から表示されます。
   </p><p>
<!--
    In some cases it might also be necessary to increase
    <varname>SEMMAP</varname> to be at least on the order of
    <varname>SEMMNS</varname>.  If the system has this parameter
    (many do not), it defines the size of the semaphore
    resource map, in which each contiguous block of available semaphores
    needs an entry. When a semaphore set is freed it is either added to
    an existing entry that is adjacent to the freed block or it is
    registered under a new map entry. If the map is full, the freed
    semaphores get lost (until reboot). Fragmentation of the semaphore
    space could over time lead to fewer available semaphores than there
    should be.
-->
場合によっては<code class="varname">SEMMAP</code>を少なくとも<code class="varname">SEMMNS</code>と同程度に増やすことが必要になる場合があるかもしれません。
システムにこのパラメータがあるなら（ないかもしれません）、このパラメータはセマフォリソースマップのサイズを定義し、その中では有効なセマフォのそれぞれの隣接したブロックの項目が必要です。
セマフォ集合が解放されると、解放されたブロックに隣接する既に存在する項目に追加されるか、もしくは新しいマップの項目の下に登録されます。
もしマップが一杯だった場合、解放されたセマフォは（再起動するまで）失われます。
セマフォ空間の断片化により時間が経つごとに、有効なセマフォがあるべき量よりも少なくなる可能性があります。
   </p><p>
<!--
    Various other settings related to <quote>semaphore undo</quote>, such as
    <varname>SEMMNU</varname> and <varname>SEMUME</varname>, do not affect
    <productname>PostgreSQL</productname>.
-->
<code class="varname">SEMMNU</code>と<code class="varname">SEMUME</code>のような、その他の様々な<span class="quote">「<span class="quote">semaphore undo</span>」</span>に関する設定は<span class="productname">PostgreSQL</span>には影響を与えません。
   </p><p>
<!--
    When using POSIX semaphores, the number of semaphores needed is the
    same as for System V, that is one semaphore per allowed connection
    (<xref linkend="guc-max-connections"/>), allowed autovacuum worker process
    (<xref linkend="guc-autovacuum-max-workers"/>) and allowed background
    process (<xref linkend="guc-max-worker-processes"/>).
    On the platforms where this option is preferred, there is no specific
    kernel limit on the number of POSIX semaphores.
-->
POSIXセマフォを使用している場合、System Vと同じ数のセマフォを必要とします。
つまり、許可した接続（<a class="xref" href="runtime-config-connection.html#GUC-MAX-CONNECTIONS">max_connections</a>）、許可したオートバキュームワーカープロセス（<a class="xref" href="runtime-config-autovacuum.html#GUC-AUTOVACUUM-MAX-WORKERS">autovacuum_max_workers</a>）、許可したバックエンドプロセス(<a class="xref" href="runtime-config-resource.html#GUC-MAX-WORKER-PROCESSES">max_worker_processes</a>)ごとに1つのセマフォを使用します。
このオプションが優先されるプラットフォームでは、POSIXセマフォの数に特定のカーネル制限はありません。
   </p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><span class="systemitem">AIX</span>
      <a id="id-1.6.6.7.3.16.1.1.2" class="indexterm"></a>
      <a id="id-1.6.6.7.3.16.1.1.3" class="indexterm"></a>
      </span></dt><dd><p>
<!--
        It should not be necessary to do
        any special configuration for such parameters as
        <varname>SHMMAX</varname>, as it appears this is configured to
        allow all memory to be used as shared memory.  That is the
        sort of configuration commonly used for other databases such
        as <application>DB/2</application>.</para>
-->
<code class="varname">SHMMAX</code>などのパラメータに対して特別な設定は必要ありません。
これは、すべてのメモリを共有メモリとして使用できるように設定されているためです。
これは<span class="application">DB/2</span>などの他のデータベースでも使用される、一般的な設定方法です。
       </p><!--
       <para> It might, however, be necessary to modify the global
       <command>ulimit</command> information in
       <filename>/etc/security/limits</filename>, as the default hard
       limits for file sizes (<varname>fsize</varname>) and numbers of
       files (<varname>nofiles</varname>) might be too low.
--><p>
しかし、<code class="filename">/etc/security/limits</code>内の大域的な<code class="command">ulimit</code>情報は変更しなければならないかもしれません。
デフォルトのファイルサイズ（<code class="varname">fsize</code>）とファイル数（<code class="varname">nofiles</code>）用のハードリミットは低過ぎるかもしれないためです。
       </p></dd><dt><span class="term"><span class="systemitem">FreeBSD</span>
      <a id="id-1.6.6.7.3.16.2.1.2" class="indexterm"></a>
      <a id="id-1.6.6.7.3.16.2.1.3" class="indexterm"></a>
      </span></dt><dd><p>
<!--
        The default shared memory settings are usually good enough, unless
        you have set <literal>shared_memory_type</literal> to <literal>sysv</literal>.
        System V semaphores are not used on this platform.
-->
<code class="literal">shared_memory_type</code>を<code class="literal">sysv</code>に設定していない限り、通常はデフォルトの共有メモリ設定で十分です。
System V セマフォはこのプラットフォームでは使用しません。
       </p><p>
<!--
        The default IPC settings can be changed using
        the <command>sysctl</command> or
        <command>loader</command> interfaces.  The following
        parameters can be set using <command>sysctl</command>:
-->
デフォルトのIPC設定は<code class="command">sysctl</code>または<code class="command">loader</code>インタフェースを使用して変更を行うことができます。
以下では<code class="command">sysctl</code>を使用してパラメータを変更しています。
</p><pre class="screen">
<code class="prompt">#</code> <strong class="userinput"><code>sysctl kern.ipc.shmall=32768</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>sysctl kern.ipc.shmmax=134217728</code></strong>
</pre><p>
<!--
        To make these settings persist over reboots, modify
        <filename>/etc/sysctl.conf</filename>.
-->
これらの設定を再起動しても永続化するには、<code class="filename">/etc/sysctl.conf</code>を変更します。
       </p><p>
<!--
        If you have set <literal>shared_memory_type</literal> to
        <literal>sysv</literal>, you might also want to configure your kernel
        to lock System V shared memory into RAM and prevent it from being paged
        out to swap.  This can be accomplished using the <command>sysctl</command>
        setting <literal>kern.ipc.shm_use_phys</literal>.
-->
<code class="literal">shared_memory_type</code>を<code class="literal">sysv</code>に設定している場合は、System V共有メモリをRAM上に固定して、スワップによってページアウトされるのを避けるために、カーネルを設定することもできます。
これは<code class="command">sysctl</code>を使用して<code class="literal">kern.ipc.shm_use_phys</code>を設定することで実現できます。
       </p><p>
<!--
        If running in a FreeBSD jail, you should set its
        <literal>sysvshm</literal> parameter to <literal>new</literal>, so that
        it has its own separate System V shared memory namespace.
        (Before FreeBSD 11.0, it was necessary to enable shared access to
        the host's IPC namespace from jails, and take measures to avoid
        collisions.)
-->
FreeBSD jailを実行している場合、<code class="literal">sysvshm</code>パラメータを<code class="literal">new</code>に設定して、独自のSystem V共有メモリ名前空間を有するようにする必要があります。
（FreeBSD 11.0以前は、jailからIPC名前空間への共有アクセスを可能にし、衝突を避けるための対策を講じる必要がありました。）
       </p></dd><dt><span class="term"><span class="systemitem">NetBSD</span>
      <a id="id-1.6.6.7.3.16.3.1.2" class="indexterm"></a>
      <a id="id-1.6.6.7.3.16.3.1.3" class="indexterm"></a>
      </span></dt><dd><p>
<!--
        The default shared memory settings are usually good enough, unless
        you have set <literal>shared_memory_type</literal> to <literal>sysv</literal>.
        You will usually want to increase <literal>kern.ipc.semmni</literal>
        and <literal>kern.ipc.semmns</literal>,
        as <systemitem class="osname">NetBSD</systemitem>'s default settings
        for these are uncomfortably small.
-->
<code class="literal">shared_memory_type</code>を<code class="literal">sysv</code>に設定していない限り、通常はデフォルトの共有メモリ設定で十分です。
通常は、<code class="literal">kern.ipc.semmni</code>と<code class="literal">kern.ipc.semmns</code>を増やす必要があります。これは、<span class="systemitem">NetBSD</span>のデフォルト設定は不適切に小さいためです。
       </p><p>
<!--
        IPC parameters can be adjusted using <command>sysctl</command>,
        for example:
-->
以下の例のようにIPCパラメータを<code class="command">sysctl</code>を用いて調整することができます。
</p><pre class="screen">
<code class="prompt">#</code> <strong class="userinput"><code>sysctl -w kern.ipc.semmni=100</code></strong>
</pre><p>
<!--
        To make these settings persist over reboots, modify
        <filename>/etc/sysctl.conf</filename>.
-->
これらの設定を再起動しても永続化するには、<code class="filename">/etc/sysctl.conf</code>を変更します。
       </p><p>
<!--
        If you have set <literal>shared_memory_type</literal> to
        <literal>sysv</literal>, you might also want to configure your kernel
        to lock System V shared memory into RAM and prevent it from being paged
        out to swap.  This can be accomplished using the <command>sysctl</command>
        setting <literal>kern.ipc.shm_use_phys</literal>.
-->
<code class="literal">shared_memory_type</code>を<code class="literal">sysv</code>に設定している場合は、System V共有メモリをRAM上に固定して、スワップによってページアウトされるのを避けるために、カーネルを設定することもできます。
これは<code class="command">sysctl</code>を使用して<code class="literal">kern.ipc.shm_use_phys</code>を設定することで実現できます。
       </p></dd><dt><span class="term"><span class="systemitem">OpenBSD</span>
      <a id="id-1.6.6.7.3.16.4.1.2" class="indexterm"></a>
      <a id="id-1.6.6.7.3.16.4.1.3" class="indexterm"></a>
      </span></dt><dd><p>
<!--
        The default shared memory settings are usually good enough, unless
        you have set <literal>shared_memory_type</literal> to <literal>sysv</literal>.
        You will usually want to
        increase <literal>kern.seminfo.semmni</literal>
        and <literal>kern.seminfo.semmns</literal>,
        as <systemitem class="osname">OpenBSD</systemitem>'s default settings
        for these are uncomfortably small.
-->
<code class="literal">shared_memory_type</code>を<code class="literal">sysv</code>に設定していない限り、通常はデフォルトの共有メモリ設定で十分です。
通常は、<code class="literal">kern.seminfo.semmni</code>と<code class="literal">kern.seminfo.semmns</code>を増やす必要があります。これは、<span class="systemitem">OpenBSD</span>のデフォルト設定は不適切に小さいためです。
       </p><p>
<!--
        IPC parameters can be adjusted using <command>sysctl</command>,
        for example:
-->
以下の例のようにIPCパラメータを<code class="command">sysctl</code>を用いて調整することができます。
</p><pre class="screen">
<code class="prompt">#</code> <strong class="userinput"><code>sysctl kern.seminfo.semmni=100</code></strong>
</pre><p>
<!--
        To make these settings persist over reboots, modify
        <filename>/etc/sysctl.conf</filename>.
-->
これらの設定を再起動しても永続化するには、<code class="filename">/etc/sysctl.conf</code>を変更します。
       </p></dd><dt><span class="term"><span class="systemitem">Linux</span>
      <a id="id-1.6.6.7.3.16.5.1.2" class="indexterm"></a>
      <a id="id-1.6.6.7.3.16.5.1.3" class="indexterm"></a>
      </span></dt><dd><p>
<!--
        The default shared memory settings are usually good enough, unless
        you have set <literal>shared_memory_type</literal> to <literal>sysv</literal>,
        and even then only on older kernel versions that shipped with low defaults.
        System V semaphores are not used on this platform.
-->
<code class="literal">shared_memory_type</code>を<code class="literal">sysv</code>に設定していて、低いデフォルトで出荷されたより古いカーネルバージョンでない限り、通常はデフォルトの共有メモリ設定で十分です。
System V セマフォはこのプラットフォームでは使用しません。
       </p><p>
<!--
        The shared memory size settings can be changed via the
        <command>sysctl</command> interface.  For example, to allow 16 GB:
-->
共有メモリサイズの設定は<code class="command">sysctl</code>インタフェースを使用して変更可能です。
例えば16ギガバイトまで許すには以下のようにします。
</p><pre class="screen">
<code class="prompt">$</code> <strong class="userinput"><code>sysctl -w kernel.shmmax=17179869184</code></strong>
<code class="prompt">$</code> <strong class="userinput"><code>sysctl -w kernel.shmall=4194304</code></strong>
</pre><p>
<!--
        To make these settings persist over reboots, see
        <filename>/etc/sysctl.conf</filename>.
-->
これらの設定を再起動しても永続化するには、<code class="filename">/etc/sysctl.conf</code>を参照してください。
       </p></dd><dt><span class="term"><span class="systemitem">macOS</span>
      <a id="id-1.6.6.7.3.16.6.1.2" class="indexterm"></a>
      <a id="id-1.6.6.7.3.16.6.1.3" class="indexterm"></a>
      </span></dt><dd><p>
<!--
        The default shared memory and semaphore settings are usually good enough, unless
        you have set <literal>shared_memory_type</literal> to <literal>sysv</literal>.
-->
<code class="literal">shared_memory_type</code>を<code class="literal">sysv</code>に設定していない限り、通常はデフォルトの共有メモリとセマフォ設定で十分です。
       </p><p>
<!--
        The recommended method for configuring shared memory in macOS
        is to create a file named <filename>/etc/sysctl.conf</filename>,
        containing variable assignments such as:
-->
macOSにおける共有メモリの推奨設定方法は、以下のような変数代入文からなる<code class="filename">/etc/sysctl.conf</code>という名称のファイルを作成することです。
</p><pre class="programlisting">
kern.sysv.shmmax=4194304
kern.sysv.shmmin=1
kern.sysv.shmmni=32
kern.sysv.shmseg=8
kern.sysv.shmall=1024
</pre><p>
<!--
        Note that in some macOS versions,
        <emphasis>all five</emphasis> shared-memory parameters must be set in
        <filename>/etc/sysctl.conf</filename>, else the values will be ignored.
-->
一部のバージョンのmacOSでは<code class="filename">/etc/sysctl.conf</code>内に共有メモリパラメータ<span class="emphasis"><em>5つすべて</em></span>を設定しなければならないという点に注意してください。
さもなくば値が無視されます。
       </p><p>
<!--
        <varname>SHMMAX</varname> can only be set to a multiple of 4096.
-->
<code class="varname">SHMMAX</code>は4096の倍数のみ設定できます。
       </p><p>
<!--
        <varname>SHMALL</varname> is measured in 4 kB pages on this platform.
-->
このプラットフォームでは<code class="varname">SHMALL</code>は4キロバイトページ単位です。
       </p><p>
<!--
        It is possible to change all but <varname>SHMMNI</varname> on the fly, using
        <application>sysctl</application>.  But it's still best to set up your preferred
        values via <filename>/etc/sysctl.conf</filename>, so that the values will be
        kept across reboots.
-->
<code class="varname">SHMMNI</code>以外の変更は、<span class="application">sysctl</span>を用いることにより、その場でおこなうことができます。
しかしいずれにせよ<code class="filename">/etc/sysctl.conf</code>経由で望む値に設定することが最善です。
再起動を行っても値が保持されるからです。
       </p></dd><dt><span class="term"><span class="systemitem">Solaris</span><br /></span><span class="term"><span class="systemitem">illumos</span></span></dt><dd><p>
<!--
        The default shared memory and semaphore settings are usually good enough for most
        <productname>PostgreSQL</productname> applications.  Solaris defaults
        to a <varname>SHMMAX</varname> of one-quarter of system <acronym>RAM</acronym>.
        To further adjust this setting, use a project setting associated
        with the <literal>postgres</literal> user.  For example, run the
        following as <literal>root</literal>:
-->
大抵の<span class="productname">PostgreSQL</span>アプリケーションではデフォルトの共有メモリとセマフォ設定で十分です。
Solarisのデフォルトの<code class="varname">SHMMAX</code>はシステムの<acronym class="acronym">RAM</acronym>の1/4になりました。
さらにこの設定を調整するためには、<code class="literal">postgres</code>ユーザに関するプロジェクト設定を使用しなければなりません。
例えば以下を<code class="literal">root</code>権限で実行してください。
</p><pre class="programlisting">
projadd -c "PostgreSQL DB User" -K "project.max-shm-memory=(privileged,8GB,deny)" -U postgres -G postgres user.postgres
</pre><p>
       </p><p>
<!--
        This command adds the <literal>user.postgres</literal> project and
        sets the shared memory maximum for the <literal>postgres</literal>
        user to 8GB, and takes effect the next time that user logs
        in, or when you restart <productname>PostgreSQL</productname> (not reload).
        The above assumes that <productname>PostgreSQL</productname> is run by
        the <literal>postgres</literal> user in the <literal>postgres</literal>
        group.  No server reboot is required.
-->
このコマンドは<code class="literal">user.postgres</code>プロジェクトを追加し、<code class="literal">postgres</code>ユーザの共有メモリの最大サイズを8GBに設定します。
この影響は次にこのユーザがログインした時、または<span class="productname">PostgreSQL</span>を再起動した時（再読み込み時ではありません）に有効になります。
上では<span class="productname">PostgreSQL</span>は<code class="literal">postgres</code>グループに属する<code class="literal">postgres</code>ユーザにより実行されていることを前提としています。
サーバの再起動は不要です。
       </p><p>
<!--
        Other recommended kernel setting changes for database servers which will
        have a large number of connections are:
-->
多くの接続を受け付けるデータベースサーバにおいて推奨するカーネル設定にはこの他に以下があります。
</p><pre class="programlisting">
project.max-shm-ids=(priv,32768,deny)
project.max-sem-ids=(priv,4096,deny)
project.max-msg-ids=(priv,4096,deny)
</pre><p>
       </p><p>
<!--
        Additionally, if you are running <productname>PostgreSQL</productname>
        inside a zone, you may need to raise the zone resource usage
        limits as well.  See "Chapter2:  Projects and Tasks" in the
        <citetitle>System Administrator's Guide</citetitle> for more
        information on <literal>projects</literal> and <command>prctl</command>.
-->
さらに、ゾーン内で<span class="productname">PostgreSQL</span>を実行している場合、ゾーンのリソース使用上限も上げる必要があるかもしれません。
<code class="literal">projects</code>と<code class="command">prctl</code>については<em class="citetitle">System Administrator's Guide</em>の第2章 プロジェクトとタスクを参照してください。
       </p></dd></dl></div></div><div class="sect2" id="SYSTEMD-REMOVEIPC"><div class="titlepage"><div><div><h3 class="title">19.4.2. systemd RemoveIPC <a href="#SYSTEMD-REMOVEIPC" class="id_link">#</a></h3></div></div></div><a id="id-1.6.6.7.4.2" class="indexterm"></a><p>
<!--
    If <productname>systemd</productname> is in use, some care must be taken
    that IPC resources (including shared memory) are not prematurely
    removed by the operating system.  This is especially of concern when
    installing PostgreSQL from source.  Users of distribution packages of
    PostgreSQL are less likely to be affected, as
    the <literal>postgres</literal> user is then normally created as a system
    user.
-->
<span class="productname">systemd</span>が使用されている場合、（共有メモリを含む）IPCリソースがオペレーティングシステムによって時期尚早に削除されないように注意する必要があります。
これはPostgreSQLをソースからインストールした場合に特に重要です。
PostgreSQLのディストリビューションパッケージのユーザーは、通常<code class="literal">postgres</code>ユーザーがシステムユーザーで作成されるため、影響を受けにくいでしょう。
   </p><p>
<!--
    The setting <literal>RemoveIPC</literal>
    in <filename>logind.conf</filename> controls whether IPC objects are
    removed when a user fully logs out.  System users are exempt.  This
    setting defaults to on in stock <productname>systemd</productname>, but
    some operating system distributions default it to off.
-->
<code class="filename">logind.conf</code>の<code class="literal">RemoveIPC</code>の設定はユーザが完全にログアウトしたときにIPCオブジェクトを削除するかどうかを制御します。
システムユーザは免除されます。
この設定のデフォルトは<span class="productname">systemd</span>ですが、いくつかのオペレーティングシステムではデフォルトでオフになっています。
   </p><p>
<!--
    A typical observed effect when this setting is on is that shared memory
    objects used for parallel query execution are removed at apparently random
    times, leading to errors and warnings while attempting to open and remove
    them, like
-->
この設定が有効になっている時の典型的な影響は、並列問い合わせの実行で使われる共有メモリオブジェクトが見かけ上ランダムな時間に削除され、共有メモリオブジェクトをオープンしようとしたり、削除しようとしたりした時に以下のようなエラーや警告が出ることです。
</p><pre class="screen">
WARNING:  could not remove shared memory segment "/PostgreSQL.1450751626": No such file or directory
</pre><p>
<!--
    Different types of IPC objects (shared memory vs. semaphores, System V
    vs. POSIX) are treated slightly differently
    by <productname>systemd</productname>, so one might observe that some IPC
    resources are not removed in the same way as others.  But it is not
    advisable to rely on these subtle differences.
-->
IPCオブジェクトの違い（共有メモリ vs. セマフォ、System V vs. POSIX）は<span class="productname">systemd</span>によって若干扱いが異なるため一部のIPCは他のものと違って削除されないことがあります。
しかし、これらの微妙な違いに依存することはお勧めできません。
   </p><p>
<!--
    A <quote>user logging out</quote> might happen as part of a maintenance
    job or manually when an administrator logs in as
    the <literal>postgres</literal> user or something similar, so it is hard
    to prevent in general.
-->
<span class="quote">「<span class="quote">ユーザーログアウト</span>」</span>は、メンテナンスジョブの一環として、又は手動で、管理者が<code class="literal">postgres</code>ユーザーや類似のユーザでログインする可能性があるため、一般的に防止することは困難です。
   </p><p>
<!--
    What is a <quote>system user</quote> is determined
    at <productname>systemd</productname> compile time from
    the <symbol>SYS_UID_MAX</symbol> setting
    in <filename>/etc/login.defs</filename>.
-->
<span class="quote">「<span class="quote">システムユーザー</span>」</span>は、<code class="filename">/etc/login.defs</code>の<code class="symbol">SYS_UID_MAX</code>の設定により<span class="productname">systemd</span>のコンパイル時に決定されます。
   </p><p>
<!--
    Packaging and deployment scripts should be careful to create
    the <literal>postgres</literal> user as a system user by
    using <literal>useradd -r</literal>, <literal>adduser &#45;-system</literal>,
    or equivalent.
-->
パッケージとデプロイスクリプトは、<code class="literal">useradd -r</code>, <code class="literal">adduser --system</code>又は同等のコマンドを使用して<code class="literal">postgres</code>ユーザを作成するように注意する必要があります。
   </p><p>
<!--
    Alternatively, if the user account was created incorrectly or cannot be
    changed, it is recommended to set
-->
また、ユーザアカウントが誤って作成されて変更出来ないような場合は、以下を設定することを推奨します。
</p><pre class="programlisting">
RemoveIPC=no
</pre><p>
<!--
    in <filename>/etc/systemd/logind.conf</filename> or another appropriate
    configuration file.
-->
<code class="filename">/etc/systemd/logind.conf</code>又はその他の設定ファイルで上記を入れます。
   </p><div class="caution"><h3 class="title">注意</h3><p>
<!--
     At least one of these two things has to be ensured, or the PostgreSQL
     server will be very unreliable.
-->
これらの２つのうち少なくとも１つが保証されてないとなりません。そうでないとPostgreSQLサーバは非常に信頼性が低くなります。
    </p></div></div><div class="sect2" id="KERNEL-RESOURCES-LIMITS"><div class="titlepage"><div><div><h3 class="title">19.4.3. リソースの制限 <a href="#KERNEL-RESOURCES-LIMITS" class="id_link">#</a></h3></div></div></div><!--
   <title>Resource Limits</title>
--><p>
<!--
    Unix-like operating systems enforce various kinds of resource limits
    that might interfere with the operation of your
    <productname>PostgreSQL</productname> server. Of particular
    importance are limits on the number of processes per user, the
    number of open files per process, and the amount of memory available
    to each process. Each of these have a <quote>hard</quote> and a
    <quote>soft</quote> limit. The soft limit is what actually counts
    but it can be changed by the user up to the hard limit. The hard
    limit can only be changed by the root user. The system call
    <function>setrlimit</function> is responsible for setting these
    parameters. The shell's built-in command <command>ulimit</command>
    (Bourne shells) or <command>limit</command> (<application>csh</application>) is
    used to control the resource limits from the command line. On
    BSD-derived systems the file <filename>/etc/login.conf</filename>
    controls the various resource limits set during login. See the
    operating system documentation for details. The relevant
    parameters are <varname>maxproc</varname>,
    <varname>openfiles</varname>, and <varname>datasize</varname>. For
    example:
-->
Unixライクなオペレーティングシステムでは<span class="productname">PostgreSQL</span>サーバの操作と関係する可能性のある様々な種類のリソース制限があります。
特に重要なのは、ユーザごとのプロセス数の制限、プロセスごとのオープンファイルの数、プロセスごとの利用可能なメモリの量です。
これらのそれぞれが<span class="quote">「<span class="quote">ハード</span>」</span>と<span class="quote">「<span class="quote">ソフト</span>」</span>の2つの制限を持っています。
ソフト制限が実際に有効な制限ですが、ユーザによってハード制限まで変えることが可能です。
ハード制限はrootユーザによってのみ変えることができます。
<code class="function">setrlimit</code>システムコールがこれらのパラメータの設定を行います。
シェルの組み込みコマンド<code class="command">ulimit</code>（Bourne シェル）もしくは<code class="command">limit</code>（<span class="application">csh</span>）は、コマンドラインからリソース制限を制御するために使われます。
BSD派生システム上では<code class="filename">/etc/login.conf</code>ファイルが、ログイン時に設定される様々なリソース制限を制御します。
詳細はオペレーティングシステムの文書を参照してください。
関連するパラメータは<code class="varname">maxproc</code>、<code class="varname">openfiles</code>、<code class="varname">datasize</code>です。
以下に例を示します。
</p><pre class="programlisting">
default:\
...
        :datasize-cur=256M:\
        :maxproc-cur=256:\
        :openfiles-cur=256:\
...
</pre><p>
<!--
    (<literal>-cur</literal> is the soft limit.  Append
    <literal>-max</literal> to set the hard limit.)
-->
（<code class="literal">-cur</code>はソフト制限です。
ハード制限を設定するためには<code class="literal">-max</code>を付けてください。）
   </p><p>
<!--
    Kernels can also have system-wide limits on some resources.
-->
カーネルはいくつかのリソースに対して、システム全体の制限も持つことができます。
    </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
<!--
      On <productname>Linux</productname> the kernel parameter
      <varname>fs.file-max</varname> determines the maximum number of open
      files that the kernel will support.  It can be changed with
      <literal>sysctl -w fs.file-max=<replaceable>N</replaceable></literal>.
      To make the setting persist across reboots, add an assignment
      in <filename>/etc/sysctl.conf</filename>.
      The maximum limit of files per process is fixed at the time the
      kernel is compiled; see
      <filename>/usr/src/linux/Documentation/proc.txt</filename> for
      more information.
-->
<span class="productname">Linux</span>カーネルパラメータ<code class="varname">fs.file-max</code>は、カーネルがサポートするオープンファイルの最大数を決定します。
これは<code class="literal">sysctl-w fs.file-max=<em class="replaceable"><code>N</code></em></code>で変更できます。
再起動後もこの設定を保持するには、<code class="filename">/etc/sysctl.conf</code>に割り当てを追加します。
プロセスあたりのファイル数の上限は、カーネルのコンパイル時に固定されます。
詳細は<code class="filename">/usr/src/linux/Documentation/proc.txt</code>を参照してください。
      </p></li></ul></div><p>
   </p><p>
<!--
    The <productname>PostgreSQL</productname> server uses one process
    per connection so you should provide for at least as many processes
    as allowed connections, in addition to what you need for the rest
    of your system.  This is usually not a problem but if you run
    several servers on one machine things might get tight.
-->
<span class="productname">PostgreSQL</span>サーバは接続ごとに1つのプロセスを使うので、少なくとも許可された接続の数だけのプロセスに残りのシステムで必要な分を追加したものが必要になります。
通常はこれは問題ではありませんが、1つのマシン上でいくつかのサーバを起動している場合は厳しい状況になるかもしれません。
   </p><p>
<!--
    The factory default limit on open files is often set to
    <quote>socially friendly</quote> values that allow many users to
    coexist on a machine without using an inappropriate fraction of
    the system resources.  If you run many servers on a machine this
    is perhaps what you want, but on dedicated servers you might want to
    raise this limit.
-->
オープンファイルの制限の出荷時のデフォルトは、しばしば大多数のユーザはマシン上でシステムリソースの不正使用をしないという前堤に立った<span class="quote">「<span class="quote">社会的に友好的な</span>」</span>値を設定してしまいます。
もし1つのマシン上で複数のサーバを起動する場合はそれが必要でしょうが、専用サーバではこの制限を上げたいかもしれません。
   </p><p>
<!--
    On the other side of the coin, some systems allow individual
    processes to open large numbers of files; if more than a few
    processes do so then the system-wide limit can easily be exceeded.
    If you find this happening, and you do not want to alter the
    system-wide limit, you can set <productname>PostgreSQL</productname>'s <xref
    linkend="guc-max-files-per-process"/> configuration parameter to
    limit the consumption of open files.
-->
反対に、個々のプロセスが多数のファイルをオープンすることを許可するシステムもあります。
そのようなプロセスが数個以上あれば、システム全体の制限は簡単に超えてしまいます。
この発生を検知し、システム全体の制限の変更を望まない場合は、<span class="productname">PostgreSQL</span>の<a class="xref" href="runtime-config-resource.html#GUC-MAX-FILES-PER-PROCESS">max_files_per_process</a>設定パラメータを設定し、オープンファイルの消費を制限することができます。
   </p><p>
<!--
    Another kernel limit that may be of concern when supporting large
    numbers of client connections is the maximum socket connection queue
    length.  If more than that many connection requests arrive within a very
    short period, some may get rejected before the <productname>PostgreSQL</productname> server can service
    the requests, with those clients receiving unhelpful connection failure
    errors such as <quote>Resource temporarily unavailable</quote> or
    <quote>Connection refused</quote>.  The default queue length limit is 128
    on many platforms.  To raise it, adjust the appropriate kernel parameter
    via <application>sysctl</application>, then restart the <productname>PostgreSQL</productname> server.
    The parameter is variously named <varname>net.core.somaxconn</varname>
    on Linux, <varname>kern.ipc.soacceptqueue</varname> on newer FreeBSD,
    and <varname>kern.ipc.somaxconn</varname> on macOS and other BSD
    variants.
-->
多数のクライアント接続をサポートする場合に懸念されるもう1つのカーネル制限は、ソケット接続キューの最大長です。
非常に短い時間内にそれ以上の数の接続要求が到着すると、<span class="productname">PostgreSQL</span>サーバが要求を処理する前に一部が拒否される可能性があります。
これらのクライアントは<span class="quote">「<span class="quote">Resource temporarily unavailable</span>」</span>や<span class="quote">「<span class="quote">Connection refused</span>」</span>などの役に立たない接続失敗エラーを受け取ります。
多くのプラットフォームでデフォルトのキュー長制限は128です。
これを上げるには<span class="application">sysctl</span>で適切なカーネルパラメータを調整し、<span class="productname">PostgreSQL</span>サーバを再起動します。
このパラメータはLinuxでは<code class="varname">net.core.somaxconn</code>、新しいFreeBSDでは<code class="varname">kern.ipc.soacceptqueue</code>、macOSや他のBSD系では<code class="varname">kern.ipc.somaxconn</code>と様々な名前が付けられています。
   </p></div><div class="sect2" id="LINUX-MEMORY-OVERCOMMIT"><div class="titlepage"><div><div><h3 class="title">19.4.4. Linuxのメモリオーバーコミット <a href="#LINUX-MEMORY-OVERCOMMIT" class="id_link">#</a></h3></div></div></div><!--
   <title>Linux Memory Overcommit</title>
--><a id="id-1.6.6.7.6.2" class="indexterm"></a><a id="id-1.6.6.7.6.3" class="indexterm"></a><a id="id-1.6.6.7.6.4" class="indexterm"></a><a id="id-1.6.6.7.6.5" class="indexterm"></a><a id="id-1.6.6.7.6.6" class="indexterm"></a><p>
<!--
    The default virtual memory behavior on Linux is not
    optimal for <productname>PostgreSQL</productname>. Because of the
    way that the kernel implements memory overcommit, the kernel might
    terminate the <productname>PostgreSQL</productname> postmaster (the
    supervisor server process) if the memory demands of either
    <productname>PostgreSQL</productname> or another process cause the
    system to run out of virtual memory.
-->
Linuxでのデフォルトの仮想メモリの動作は<span class="productname">PostgreSQL</span>には最適ではありません。
カーネルがメモリオーバーコミットを実装する方法のため、カーネルは、<span class="productname">PostgreSQL</span>や他のプロセスのメモリ要求がシステムの仮想メモリを枯渇させた場合、<span class="productname">PostgreSQL</span> postmaster （スーパーバイザーサーバプロセス）を終了させる可能性があります。
   </p><p>
<!--
    If this happens, you will see a kernel message that looks like
    this (consult your system documentation and configuration on where
    to look for such a message):
-->
これが発生した場合、以下のようなカーネルメッセージが現れます
（こうしたメッセージを検索する場所についてはシステム文書と設定を参照してください）。
</p><pre class="programlisting">
Out of Memory: Killed process 12345 (postgres).
</pre><p>
<!--
    This indicates that the <filename>postgres</filename> process
    has been terminated due to memory pressure.
    Although existing database connections will continue to function
    normally, no new connections will be accepted.  To recover,
    <productname>PostgreSQL</productname> will need to be restarted.
-->
これは、<code class="filename">postgres</code>プロセスがメモリ不足のために終了してしまったことを示します。
起動中のデータベース接続は正常に動作しますが、新しい接続は受け付けられません。
復旧するには、<span class="productname">PostgreSQL</span>を再起動しなければなりません。
   </p><p>
<!--
    One way to avoid this problem is to run
    <productname>PostgreSQL</productname> on a machine where you can
    be sure that other processes will not run the machine out of
    memory.  If memory is tight, increasing the swap space of the
    operating system can help avoid the problem, because the
    out-of-memory (OOM) killer is invoked only when physical memory and
    swap space are exhausted.
-->
この問題を防止する1つの方法として、<span class="productname">PostgreSQL</span>を他のプロセスがそのマシンのメモリを枯渇させないことが確実なマシンで起動するというものがあります。
物理メモリとスワップ領域が消費尽くされた時のみにメモリ不足（OOM）killerが発生するため、メモリが不足する場合、オペレーティングシステムのスワップ領域を増やすことが問題解決の役にたちます。
   </p><p>
<!--
    If <productname>PostgreSQL</productname> itself is the cause of the
    system running out of memory, you can avoid the problem by changing
    your configuration.  In some cases, it may help to lower memory-related
    configuration parameters, particularly
    <link linkend="guc-shared-buffers"><varname>shared_buffers</varname></link>,
    <link linkend="guc-work-mem"><varname>work_mem</varname></link>, and
    <link linkend="guc-hash-mem-multiplier"><varname>hash_mem_multiplier</varname></link>.
    In other cases, the problem may be caused by allowing too many
    connections to the database server itself.  In many cases, it may
    be better to reduce
    <link linkend="guc-max-connections"><varname>max_connections</varname></link>
    and instead make use of external connection-pooling software.
-->
<span class="productname">PostgreSQL</span>自体が実行中のシステムのメモリ不足を引き起こした場合、設定を変更することで問題を防止することができます。
場合によっては、メモリ関連の設定パラメータ、<a class="link" href="runtime-config-resource.html#GUC-SHARED-BUFFERS"><code class="varname">shared_buffers</code></a>、<a class="link" href="runtime-config-resource.html#GUC-WORK-MEM"><code class="varname">work_mem</code></a>および<a class="link" href="runtime-config-resource.html#GUC-HASH-MEM-MULTIPLIER"><code class="varname">hash_mem_multiplier</code></a>を低くすることで回避できる場合もあります。
この他にもデータベースサーバ自体への接続を多く許可しすぎることで問題が引き起こされる場合もあります。
多くの場合、<a class="link" href="runtime-config-connection.html#GUC-MAX-CONNECTIONS"><code class="varname">max_connections</code></a>を減らし、外部のコネクションプールソフトウェアを使用することで改善されます。
   </p><p>
<!--
    It is possible to modify the
    kernel's behavior so that it will not <quote>overcommit</quote> memory.
    Although this setting will not prevent the <ulink
    url="https://lwn.net/Articles/104179/">OOM killer</ulink> from being invoked
    altogether, it will lower the chances significantly and will therefore
    lead to more robust system behavior.  This is done by selecting strict
    overcommit mode via <command>sysctl</command>:
-->
メモリを<span class="quote">「<span class="quote">オーバーコミット</span>」</span>させないようにカーネルの動作を変更することができます。
この設定は完全に<a class="ulink" href="https://lwn.net/Articles/104179/" target="_top">OOM killer</a>の発生を防ぐことはできませんが、その発生頻度をかなり軽減しますので、システム動作の堅牢性をより高めます。
これは、以下のように<code class="command">sysctl</code>を使用して厳密なオーバーコミットモードを選択するこで実施されます。
</p><pre class="programlisting">
sysctl -w vm.overcommit_memory=2
</pre><p>
<!--
    or placing an equivalent entry in <filename>/etc/sysctl.conf</filename>.
    You might also wish to modify the related setting
    <varname>vm.overcommit_ratio</varname>.  For details see the kernel documentation
    file <ulink url="https://www.kernel.org/doc/Documentation/vm/overcommit-accounting"></ulink>.
-->
または<code class="filename">/etc/sysctl.conf</code>に同等のエントリを配置します。
また、関連する<code class="varname">vm.overcommit_ratio</code>設定を変更した方が良いでしょう。
詳細はカーネル文書ファイル<a class="ulink" href="https://www.kernel.org/doc/Documentation/vm/overcommit-accounting" target="_top">https://www.kernel.org/doc/Documentation/vm/overcommit-accounting</a>を参照してください。
   </p><p>
<!--
    Another approach, which can be used with or without altering
    <varname>vm.overcommit_memory</varname>, is to set the process-specific
    <firstterm>OOM score adjustment</firstterm> value for the postmaster process to
    <literal>-1000</literal>, thereby guaranteeing it will not be targeted by the OOM
    killer.  The simplest way to do this is to execute
-->
<code class="varname">vm.overcommit_memory</code>の変更と関係なく使用できるその他の方法は、プロセス固有の<em class="firstterm">OOMスコア調整</em>値をpostmasterプロセス向けに<code class="literal">-1000</code>に設定することです。
これによりOOM killerの対象とならないことが保証されます。
このための最も簡単な方法は以下を<span class="productname">PostgreSQL</span>の起動スクリプト内で<code class="filename">postgres</code>を実行する直前に実行することです。
</p><pre class="programlisting">
echo -1000 &gt; /proc/self/oom_score_adj
</pre><p>
<!--
    in the <productname>PostgreSQL</productname> startup script just before
    invoking <filename>postgres</filename>.
    Note that this action must be done as root, or it will have no effect;
    so a root-owned startup script is the easiest place to do it.  If you
    do this, you should also set these environment variables in the startup
    script before invoking <filename>postgres</filename>:
-->
この作業をrootで実行しなければならないことに注意して下さい。
さもないと効果がありません。
このためrootが所有する起動スクリプトが、これを行うための最も簡単な場所です。
その場合には、起動スクリプトの<code class="filename">postgres</code>の起動前に以下の環境変数を設定することも推奨します。
</p><pre class="programlisting">
export PG_OOM_ADJUST_FILE=/proc/self/oom_score_adj
export PG_OOM_ADJUST_VALUE=0
</pre><p>
<!--
    These settings will cause postmaster child processes to run with the
    normal OOM score adjustment of zero, so that the OOM killer can still
    target them at need.  You could use some other value for
    <envar>PG_OOM_ADJUST_VALUE</envar> if you want the child processes to run
    with some other OOM score adjustment.  (<envar>PG_OOM_ADJUST_VALUE</envar>
    can also be omitted, in which case it defaults to zero.)  If you do not
    set <envar>PG_OOM_ADJUST_FILE</envar>, the child processes will run with the
    same OOM score adjustment as the postmaster, which is unwise since the
    whole point is to ensure that the postmaster has a preferential setting.
-->
これらの設定は、いざという時にpostmasterの子プロセスをOOM killerのターゲットに出来るようにOOMスコア調整を通常のゼロで実行します。
子プロセスを他のOOMスコア調整で実行したい場合には、<code class="envar">PG_OOM_ADJUST_VALUE</code>により別の値にすることが出来ます。
（<code class="envar">PG_OOM_ADJUST_VALUE</code>は省略することが出来て、その場合はデフォルトのゼロになります。）
<code class="envar">PG_OOM_ADJUST_FILE</code>を設定しない場合、子プロセスはpostmasterと同じOOMスコア調整で実行されますが、postmasterが優先される設定にすることが肝心なので、それは賢明とは言えません。
   </p></div><div class="sect2" id="LINUX-HUGE-PAGES"><div class="titlepage"><div><div><h3 class="title">19.4.5. LinuxのHugePages <a href="#LINUX-HUGE-PAGES" class="id_link">#</a></h3></div></div></div><!--
   <title>Linux Huge Pages</title>
--><p>
<!--
    Using huge pages reduces overhead when using large contiguous chunks of
    memory, as <productname>PostgreSQL</productname> does, particularly when
    using large values of <xref linkend="guc-shared-buffers"/>.  To use this
    feature in <productname>PostgreSQL</productname> you need a kernel
    with <varname>CONFIG_HUGETLBFS=y</varname> and
    <varname>CONFIG_HUGETLB_PAGE=y</varname>. You will also have to configure
    the operating system to provide enough huge pages of the desired size.
    To determine the number of huge pages needed, use the
    <command>postgres</command> command to see the value of
    <xref linkend="guc-shared-memory-size-in-huge-pages"/>.  Note that the
    server must be shut down to view this runtime-computed parameter.
    This might look like:
-->
<span class="productname">PostgreSQL</span>のように、メモリの大きな連続チャンクを使用するとき、特に<a class="xref" href="runtime-config-resource.html#GUC-SHARED-BUFFERS">shared_buffers</a>の値が大きい場合に、huge pagesを使用するとオーバーヘッドが減少します。
<span class="productname">PostgreSQL</span>でこの機能を有効にするには、<code class="varname">CONFIG_HUGETLBFS=y</code>および<code class="varname">CONFIG_HUGETLB_PAGE=y</code>としたカーネルが必要です。
また、要求される十分な量だけhuge pagesを提供するようにオペレーティングシステムを調整する必要があるでしょう。
必要なhuge pages数を決定するには、<code class="command">postgres</code>コマンドを起動して<a class="xref" href="runtime-config-preset.html#GUC-SHARED-MEMORY-SIZE-IN-HUGE-PAGES">shared_memory_size_in_huge_pages</a>の値を調べます。
実行時に計算されたこのパラメータを見るためには、サーバを停止しなければならないことに注意してください。
これは以下のような感じになるでしょう。
</p><pre class="programlisting">
$ <strong class="userinput"><code>postgres -D $PGDATA -C shared_memory_size_in_huge_pages</code></strong>
3170
$ <strong class="userinput"><code>grep ^Hugepagesize /proc/meminfo</code></strong>
Hugepagesize:       2048 kB
$ <strong class="userinput"><code>ls /sys/kernel/mm/hugepages</code></strong>
hugepages-1048576kB  hugepages-2048kB
</pre><p>

<!--
     In this example the default is 2MB, but you can also explicitly request
     either 2MB or 1GB with <xref linkend="guc-huge-page-size"/> to adapt
     the number of pages calculated by
     <varname>shared_memory_size_in_huge_pages</varname>.
-->
この例ではデフォルトは2MBですが、明示的に2MBまたは1GBを<a class="xref" href="runtime-config-resource.html#GUC-HUGE-PAGE-SIZE">huge_page_size</a>で要求して、<code class="varname">shared_memory_size_in_huge_pages</code>で計算されたページ数を調整することもできます。

<!--
     While we need at least <literal>3170</literal> huge pages in this example,
     a larger setting would be appropriate if other programs on the machine
     also need huge pages.
     We can set this with:
-->
この例では少なくとも<code class="literal">3170</code>のhuge pageが必要ですが、マシン上の他のプログラムもhuge pageを必要とする場合には、より大きな設定が適切です。
これは以下のように設定できます。
</p><pre class="programlisting">
# <strong class="userinput"><code>sysctl -w vm.nr_hugepages=3170</code></strong>
</pre><p>
<!--
     Don't forget to add this setting to <filename>/etc/sysctl.conf</filename>
     so that it is reapplied after reboots.  For non-default huge page sizes,
     we can instead use:
-->
再起動後にこの設定が行われるように、<code class="filename">/etc/sysctl.conf</code>にこの設定を追加するのを忘れないようにしましょう。
非標準のhuge pageサイズなら、代わりに以下のようにできます。
</p><pre class="programlisting">
# <strong class="userinput"><code>echo 3170 &gt; /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages</code></strong>
</pre><p>
<!--
    It is also possible to provide these settings at boot time using
    kernel parameters such as <literal>hugepagesz=2M hugepages=3170</literal>.
-->
<code class="literal">hugepagesz=2M hugepages=3170</code>のようにして、起動時にカーネルパラメータとしてこれらの設定を与えることもできます。
   </p><p>
<!--
    Sometimes the kernel is not able to allocate the desired number of huge
    pages immediately due to fragmentation, so it might be necessary
    to repeat the command or to reboot.  (Immediately after a reboot, most of
    the machine's memory should be available to convert into huge pages.)
    To verify the huge page allocation situation for a given size, use:
-->
時には、断片化のためにカーネルは求められた数のhuge pagesを直ちには割り当てることができないことがあるので、そのコマンドを繰り返すか、再起動する必要があるかもしれません。
（再起動の直後は、マシンのメモリの大部分はhuge pagesへの変更が可能なはずです。）
指定サイズに対するhuge pagesの割り当ての状況を確認するには、次のようにします。
</p><pre class="programlisting">
$ <strong class="userinput"><code>cat /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages</code></strong>
</pre><p>
   </p><p>
<!--
    It may also be necessary to give the database server's operating system
    user permission to use huge pages by setting
    <varname>vm.hugetlb_shm_group</varname> via <application>sysctl</application>, and/or
    give permission to lock memory with <command>ulimit -l</command>.
-->
<span class="application">sysctl</span>を使って<code class="varname">vm.hugetlb_shm_group</code>を設定する、あるいは<code class="command">ulimit -l</code>でメモリをロックする権限を与えることで、データベースサーバのOSユーザにhuge pagesを使用する権限を与える必要もあるかもしれません。
   </p><p>
<!--
    The default behavior for huge pages in
    <productname>PostgreSQL</productname> is to use them when possible, with
    the system's default huge page size, and
    to fall back to normal pages on failure. To enforce the use of huge
    pages, you can set <xref linkend="guc-huge-pages"/>
    to <literal>on</literal> in <filename>postgresql.conf</filename>.
    Note that with this setting <productname>PostgreSQL</productname> will fail to
    start if not enough huge pages are available.
-->
<span class="productname">PostgreSQL</span>のhuge pagesのデフォルトの動作は、可能な場合はシステムのデフォルトサイズのhuge pagesを使用し、失敗した場合は通常のページを使用します。
<code class="filename">postgresql.conf</code>で<a class="xref" href="runtime-config-resource.html#GUC-HUGE-PAGES">huge_pages</a>を<code class="literal">on</code>に設定することで、huge pagesの使用を強制することができます。
この設定の場合、十分なhuge pagesが確保できなければ、<span class="productname">PostgreSQL</span>の起動に失敗することに注意してください。
   </p><p>
<!--
    For a detailed description of the <productname>Linux</productname> huge
    pages feature have a look
    at <ulink url="https://www.kernel.org/doc/Documentation/vm/hugetlbpage.txt"></ulink>.
-->
<span class="productname">Linux</span>のhuge pages機能の詳細は<a class="ulink" href="https://www.kernel.org/doc/Documentation/vm/hugetlbpage.txt" target="_top">https://www.kernel.org/doc/Documentation/vm/hugetlbpage.txt</a>を参照してください。
   </p></div></div><div class="navfooter"><hr /><table width="100%" summary="Navigation footer"><tr><td width="40%" align="left"><a accesskey="p" href="server-start.html" title="19.3. データベースサーバの起動">前へ</a> </td><td width="20%" align="center"><a accesskey="u" href="runtime.html" title="第19章 サーバの準備と運用">上へ</a></td><td width="40%" align="right"> <a accesskey="n" href="server-shutdown.html" title="19.5. サーバのシャットダウン">次へ</a></td></tr><tr><td width="40%" align="left" valign="top">19.3. データベースサーバの起動 </td><td width="20%" align="center"><a accesskey="h" href="index.html" title="PostgreSQL 16.0文書">ホーム</a></td><td width="40%" align="right" valign="top"> 19.5. サーバのシャットダウン</td></tr></table></div></body></html>